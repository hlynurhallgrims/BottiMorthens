# Pakkar

library(keras)
library(rtweet)
library(tidyverse)

# Texti lesinn inn

bubbi <- rtweet::get_timeline(user = "BubbiMorthens", n = 3200)

bubbi_text <- bubbi %>% 
  pull(text) %>% 
  paste(collapse = "\n") 

text <- tolower(bubbi_text) 
cat("Corpus length:", nchar(text), "\n")

# Vigrun runa af stöfum

maxlen <- 60
step <- 3

text_indexes <- seq(1, nchar(text) - maxlen, by = step)

sentences <- str_sub(text, start = text_indexes, 
                     end = text_indexes + maxlen -1)
next_chars <- str_sub(text, text_indexes + maxlen, text_indexes + maxlen)

cat("Number of sequences: ", length(sentences), "\n")

chars <- unique(sort(strsplit(text, "")[[1]]))
cat("Unique characters:", length(chars), "\n")

char_indices <- 1:length(chars)
names(char_indices) <- chars
cat("Vectorization...\n")

x <- array(0L, dim = c(length(sentences), maxlen, length(chars))) 
y <- array(0L, dim = c(length(sentences), length(chars)))
for (i in 1:length(sentences)) {
  sentence <- strsplit(sentences[[i]], "")[[1]] 
  for (t in 1:length(sentence)) {
    char <- sentence[[t]]
    x[i, t, char_indices[[char]]] <- 1 
  }
  next_char <- next_chars[[i]]
  y[i, char_indices[[next_char]]] <- 1 
}

# Eins lags LSTM líkan sem spáir fyrir um næsta staf í runu

model <- keras_model_sequential() %>%
  layer_lstm(units = 128, input_shape = c(maxlen, length(chars))) %>%
  layer_lstm(units = 128, input_shape = c(maxlen, length(chars))) %>%
  layer_dense(units = length(chars), activation = "softmax")

# Stillum hvernig líkanið lærir

optimizer <- optimizer_rmsprop(lr = 0.01)
model %>% compile(
  loss = "categorical_crossentropy",
  optimizer = optimizer
)

# Fall fyrir sömplun á næsta staf út frá spá líkansins

sample_next_char <- function(preds, temperature = 1.0) {
  preds <- as.numeric(preds)
  preds <- log(preds) / temperature 
  exp_preds <- exp(preds)
  preds <- exp_preds / sum(exp_preds) 
  which.max(t(rmultinom(1, 1, preds)))
}

# Lúppa til útspýtingar texta

listi <- list()

for (epoch in 1:1) {
  cat("epoch", epoch, "\n")
  model %>% fit(x, y, batch_size = 128, epochs = 1)
  start_index <- sample(1:(nchar(text) - maxlen - 1), 1)
  seed_text <- str_sub(text, start_index, start_index + maxlen - 1) 
  cat("--- Generating with seed:", seed_text, "\n\n")
  for (temperature in c(0.3, 0.4, 0.5, 0.6)) {
    cat("------ temperature:", temperature, "\n")
    cat(seed_text, "\n")
    generated_text <- seed_text
    for (i in 1:400) {
      sampled <- array(0, dim = c(1, maxlen, length(chars))) 
      generated_chars <- strsplit(generated_text, "")[[1]] 
      for (t in 1:length(generated_chars)) {
        char <- generated_chars[[t]]
        sampled[1, t, char_indices[[char]]] <- 1 
      }
      preds <- model %>% predict(sampled, verbose = 0) 
      next_index <- sample_next_char(preds[1,], temperature) 
      next_char <- chars[[next_index]]
      generated_text <- paste0(generated_text, next_char) 
      generated_text <- substring(generated_text, 2)
      cat(next_char)
    }
    cat("\n\n")
    
  }
}

ConnectToApi()  # See function in ConnectToAPI.R

# send tweet
ST <- updateStatus(text = "A") # í stað "A"  á tístið sjálft úr textanum að ofan að koma
